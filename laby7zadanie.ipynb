{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pomocnicza funkcja do wyswietlania naszych danych strumieniowych\n",
    "batch_counter = {\"count\": 0}\n",
    " \n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_counter[\"count\"] % 7 == 0:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zadanie 1\n",
    "# Przygotuj strumien danych z format('rate'), ustaw rowsPerSecond na 5.\n",
    "# Utworz kolumne user_id: expr(\"concat('u', cast(rand()*100 as int))\")\n",
    "# Dodaj kolumne event_type: expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    " \n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    " \n",
    "rate_df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 5)\n",
    "      .load())\n",
    " \n",
    "events = (rate_df.withColumn(\"user_id\", expr(\"concat('u', cast(rand()*100 as int))\") )\n",
    "          .withColumn(\"event_type\", expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\") )\n",
    "          .select(\"timestamp\", \"user_id\", \"event_type\")\n",
    "         )\n",
    "query = (events.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skorzystaj z danych z poprzedniego zadania.\n",
    "# Wyfiltruj tylko purchase.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    " \n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    " \n",
    "rate_df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 5)\n",
    "      .load())\n",
    " \n",
    "events = (rate_df.withColumn(\"user_id\", expr(\"concat('u', cast(rand()*100 as int))\") )\n",
    "          .withColumn(\"event_type\", expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\") )\n",
    "          .select(\"timestamp\", \"user_id\", \"event_type\")\n",
    "         )\n",
    "\n",
    "purchases= events.filter(\"event_type = 'purchase'\")\n",
    "\n",
    "query = (\n",
    "    purchases.writeStream\n",
    "    .format(\"console\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81de3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generator.py\n"
     ]
    }
   ],
   "source": [
    "%%file generator.py\n",
    "# generator.py\n",
    "import json, os, random, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "output_dir = \"data/stream\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "event_types = [\"view\", \"cart\", \"purchase\"]\n",
    "categories = [\"electronics\", \"books\", \"fashion\", \"home\", \"sports\"]\n",
    "\n",
    "def generate_event():\n",
    "    return {\n",
    "        \"user_id\": f\"u{random.randint(1, 50)}\",\n",
    "        \"event_type\": random.choices(event_types, weights=[0.6, 0.25, 0.15])[0],\n",
    "        \"timestamp\": (datetime.utcnow() - timedelta(seconds=random.randint(0, 300))).isoformat(),\n",
    "        \"product_id\": f\"p{random.randint(100, 120)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"price\": round(random.uniform(10, 1000), 2)\n",
    "    }\n",
    "\n",
    "# Simulate file-based streaming\n",
    "while True:\n",
    "    batch = [generate_event() for _ in range(50)]\n",
    "    filename = f\"{output_dir}/events_{int(time.time())}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for e in batch:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "    print(f\"Wrote: {filename}\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa349c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz zmienną schema, która zrealizuje schamat danych naszej ramki. Wykorzystaj StringType(), TimestampType(),DoubleType()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealTimeEcommerce\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db981dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odczyt daych z katalogu\n",
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\"))\n",
    "\n",
    "query = (stream.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03362612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Przygotuj zmienną agg1 zliczającą zdarzenia należące do danej grupy event_type.\n",
    "agg1 = (stream.groupBy(\"event_type\").count())\n",
    "\n",
    "# pamietaj, że agregacje wymagają opcji complete\n",
    "query = (agg1\n",
    "         .writeStream\n",
    "         .outputMode(\"complete\")\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b75c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pogrupuj typy zdarzen w thumbling window, w oknie co 5 minut\n",
    "##dodaj watermark z ustawieniem na 1 minutę.\n",
    "windowed = (stream\n",
    "            .withWatermark(\"timestamp\", \"1 minute\") \n",
    "            .groupBy(window(\"timestamp\", \"5 minutes\"),\"event_type\")\n",
    "            .count())\n",
    "\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zmień thumbling window na sliding window z szerokością okna 5 minut i startem nowego okna co 1 minutę.\n",
    "windowed = (stream.withWatermark(\"timestamp\", \"1 minutes\")\n",
    "            .groupBy(window(\"timestamp\", \"5 minutes\", \"1 minutes\"), \"event_type\").count()\n",
    "           )\n",
    " \n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"truncate\", False) \n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "########segmentacja\n",
    "#####jeśli był purchase → \"Buyer\"\n",
    "##### jeśli był cart, ale nie purchase → \"Cart abandoner\"\n",
    "##### jeśli tylko view → \"Lurker\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_counter[\"count\"] % 7 == 0:\n",
    "        spark.stop()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealTimeEcommerce\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])\n",
    "\n",
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\")\n",
    "         )\n",
    "\n",
    "windowed = (stream.withWatermark(\"timestamp\", \"1 minutes\")\n",
    "            .groupBy(window(\"timestamp\", \"5 minutes\"),\"user_id\")\n",
    "            .agg(collect_set(\"event_type\").alias(\"event_type\"))\n",
    "            .withColumn(\"segmentacja\",expr(\"\"\"CASE\n",
    "                                            WHEN array_contains(event_type, 'purchase') THEN 'Buyer'\n",
    "                                            WHEN array_contains(event_type, 'cart') THEN 'Cart abandoner'\n",
    "                                            ELSE 'Lurker'\n",
    "                                            END\"\"\"))\n",
    "            .select(\"window\", \"user_id\", \"segmentacja\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")  \n",
    "    .option(\"truncate\", False)\n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a765922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
